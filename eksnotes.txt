#Get Kube config File for your EKS cluster:
---->The kubeconfig file is essential for interacting with your Amazon EKS cluster. This file contains configuration details such as the cluster's API server endpoint, authentication details, and context information, allowing you to use kubectl to manage your EKS cluster.
command:
aws eks --region us-east-1 update-kubeconfig --name eks-cluster
----------------------------------------------------------------------------------
#install eksctl:k
curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
----------------------------------------------------------------------------------------
#install helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

---->how to install in windows https://github.com/helm/helm/releases
--------------------------------------
#install CSI driver
#CSI drivers allow Kubernetes to interface with various storage backends through a standard API. 

helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver

helm repo update
helm upgrade --install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver --namespace kube-system --version v1.33.0-eksbuild.1

-----------------------------
to decrypt base64
--------------------------
powershell -Command "[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String('WEVWZkpvbkxPTTVKb3hKajFwcjZxSHY4V1c3RFdVUWdtemVTcnlMVA=='))"

Encoding to base64 in windows

$text = "password"
$bytes = [System.Text.Encoding]::UTF8.GetBytes($text)
$base64 = [Convert]::ToBase64String($bytes)
$base64

--------------------------------------
ingress nginix install

Webpage ref:https://kubernetes.github.io/ingress-nginx/deploy/


-------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-petclinic-dev
  namespace: petclinic-dev
spec:
  rules:
    - host: "a31ecb4d8de0a4e8c805c6a706e12f50-860764022.us-east-1.elb.amazonaws.com" # lb dns name as i don't have dns name
      http:
        paths:
        - pathType: Prefix
          path: "/dev"
          backend:         
            service:
              name: frontend-service-dev
              port:
                number: 80
              #This configuration ensures that requests to the path /dev on the external IP address of the Ingress Controller will be routed to the frontend-service-dev service, even though it is a ClusterIP service.

              =------------------------------------------------------

installing nginx ingress controller using helm 

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx/
helm repo update
kubectl create namespace ingress-nginx
helm install nginx-ingress ingress-nginx/ingress-nginx -n ingress-nginx

make sure controller watches all ingress rescources in all the namespaces 
modify this deployment.yml of controller:

kubectl edit deployment nginx-ingress-ingress-nginx-controller -n ingress-nginx  

spec:
  containers:
    - name: nginx-ingress-controller
      args:
        - /nginx-ingress-controller
        - --watch-namespace=

        add  - --watch-namespace=
-----
if you find any issue  make sure to check the logs of the deployment
kubectl logs nginx-ingress-ingress-nginx-controller-7855544b44-n6bcf -n ingress-nginx


--------------------------------------------------------
IRSA: ---> to make eks to access ebs 

install eks then

associate oidc provider with iam:
eksctl utils associate-iam-oidc-provider --cluster eks-cluster --approve

then create an iam role for service account 

single line:
eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster eks-cluster --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve --role-only --role-name AmazonEKS_EBS_CSI_DriverRole


eksctl create addon --name aws-ebs-csi-driver --cluster eks-cluster --service-account-role-arn arn:aws:iam::021891605639:role/AmazonEKS_EBS_CSI_DriverRole --force


as the IRSA GOT depriciated for ebs_csi_aws we need to use pod identily associate
 (IRSA has been deprecated; the recommended way to provide IAM permissions for "aws-ebs-csi-driver" addon is via pod identity associations; after addon creation is completed, run `eksctl utils migrate-to-pod-identity`)



 eksctl utils migrate-to-pod-identity --cluster eks-cluster --region us-east-1   --approve




if we have this below error follow the below commands

PS C:\Users\prasa> eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster eks-cluster --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve --role-only --role-name AmazonEKS_EBS_CSI_DriverRole
2024-08-15 09:48:58 [ℹ]  1 existing iamserviceaccount(s) (kube-system/ebs-csi-controller-sa) will be excluded
2024-08-15 09:48:58 [ℹ]  1 iamserviceaccount (kube-system/ebs-csi-controller-sa) was excluded (based on the include/exclude rules)
2024-08-15 09:48:58 [!]  serviceaccounts in Kubernetes will not be created or modified, since the option --role-only is used
2024-08-15 09:48:58 [ℹ]  no tasks

eksctl delete iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster eks-cluster 
 eksctl delete addon --name aws-ebs-csi-driver --cluster eks-cluster
aws eks describe-addon-versions --addon-name aws-ebs-csi-driver --kubernetes-version 1.30










----------------------------
kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm

nslookup mysql-headless-backend.petclinic-dev.svc.cluster.local
nslookup frontend-service-dev.petclinic-dev.svc.cluster.local

-----------------------
kubectl run -i --tty --image nginx:alpine test-pod --restart=Never --rm -- sh

curl http://frontend-service-dev.petclinic-dev.svc.cluster.local:81

we cannot use curl to connect to mysql
MySQL is a database server and does not communicate over HTTP by default; it uses its own protocol over TCP.
or use netcat:
nc -zv mysql-headless-backend.petclinic-dev.svc.cluster.local 3306

-----------------------------------------
instead use 
kubectl run -i --tty --image=mysql:5.0 --restart=Never --rm mysql-client -- sh

mysql -h mysql-headless-backend.petclinic-dev.svc.cluster.local -P 3306 -u your-username -p

--------------------------------------------
copy data from local machine to pods pv mount path 

kubectl cp ./mysql mysql-0:/var/lib/mysql

kubectl cp . mysql-0:/var/lib/mysql -c mysql -n petclinic-prod

enter into mysql container 
kubectl exec -it mysql-0 -n petclinic-prod -- sh
kubectl exec -it mysql-0 -n petclinic-dev -- sh

then connect to mysql client 
bash-4.2# mysql -u root -p
Enter password:

RUN THIS COMMANDS AND CREATE TABELES AND DATA IN IT 
SOURCE user.sql
USE petclinic;
SOURCE data.sql
SOURCE schema.sql
verify using below:
SHOW DATABASES;
USE petclinic;
show TABELES;
SELECT * FROM owners;
SELECT * FROM vets;

to check data is stored in pv restart the pod and rerun the following commands using mysql client
---------------------------------------------------
use this if we get an issue stating that unable to create the pod and connect to pv as there are already files in the mentioned path 
inorder to clear the files create and init container which runs before the pod and clear the files in that specific path
initContainers:
        - name: init-mysql-cleanup
          image: busybox
          command: ['sh', '-c', 'rm -rf /var/lib/mysql/*']
          volumeMounts:
            - name: mysql-storage-prod
              mountPath: /var/lib/mysql

this helps in clearing the pv dir files  if the issue happens 
---------------------------------------------------

got an issue like when ever i restart my frontend pod all the data i entered is deleting it is not becouse the backend is not correctly configered 
but application.properties Database is configered as H2 which is in-memory database which is not persistent whenever pod restarted data will be lost
so i changed it to MYSQL and so data is now getting stored in pv even if any frentend or backend pod get restarted data will never get lost until we delete the pv manully 
# Activate the MySQL profile this will change the default H2 to MYSQL same need to change to any other databases 
spring.profiles.active=mysql

and missconfigured the svc in deployoment manifest resulting in prod rescources saving data in dev mysql to over come this we neede to use RBAC
Even if we missconfigured we need to male sure that dev rescources should not communicate with prod rescources.

make sure to  config configmaps and servers in order to get error 

this we faces:
ImagePullBackOff
CrashLoopBackOff
ConfigMapError
Createcontainerconfigerror

Prometeus ans grafana:
----------------------
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.prometheus.svc.cluster.local
Prometheus server URL
kubectl --namespace prometheus port-forward $POD_NAME 9090
alert manager
kubectl --namespace prometheus port-forward $POD_NAME 9093
The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.prometheus.svc.cluster.local
 PushGateway URL 
 kubectl --namespace prometheus port-forward $POD_NAME 9091

 ------------------------
 this command checks the cpu and memory ultilization of nodes 
C:\Users\prasa>kubectl top nodes
NAME                         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-0-3-230.ec2.internal   34m          1%     1318Mi          39%
ip-10-0-4-216.ec2.internal   28m          1%     1124Mi          33%

#we need to make sure to have actuator dependency is Spring Boot Actuator provides production-ready features like monitoring, metrics, and health checks for your Spring Boot applications.
several built-in endpoints that offer valuable insights into the health and performance of your application. These endpoints can be accessed over HTTP and are very useful for monitoring and managing your application.
<dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>

and we also need to add the below command in application.properties file  to enable all the metric end points

# Actuator  will enable all th metrics endpoints 
management.endpoints.web.exposure.include=*

this is are all the default metric views available 
to view this use 
URL:PORT/actuator/metrics

{"names":["application.ready.time","application.started.time","disk.free","disk.total","executor.active","executor.completed","executor.pool.core",
"executor.pool.max","executor.pool.size","executor.queue.remaining","executor.queued","hikaricp.connections","hikaricp.connections.acquire",
"hikaricp.connections.active","hikaricp.connections.creation","hikaricp.connections.idle","hikaricp.connections.max","hikaricp.connections.min",
"hikaricp.connections.pending","hikaricp.connections.timeout","hikaricp.connections.usage","http.server.requests.active","jdbc.connections.active",
"jdbc.connections.idle","jdbc.connections.max","jdbc.connections.min","jvm.buffer.count","jvm.buffer.memory.used","jvm.buffer.total.capacity",
"jvm.classes.loaded","jvm.classes.unloaded","jvm.compilation.time","jvm.gc.live.data.size","jvm.gc.max.data.size","jvm.gc.memory.allocated",
"jvm.gc.memory.promoted","jvm.gc.overhead","jvm.gc.pause","jvm.info","jvm.memory.committed","jvm.memory.max","jvm.memory.usage.after.gc",
"jvm.memory.used","jvm.threads.daemon","jvm.threads.live","jvm.threads.peak","jvm.threads.started","jvm.threads.states","logback.events",
"process.cpu.time","process.cpu.usage","process.files.max","process.files.open","process.start.time","process.uptime","system.cpu.count",
"system.cpu.usage","system.load.average.1m","tomcat.sessions.active.current","tomcat.sessions.active.max","tomcat.sessions.alive.max",
"tomcat.sessions.created","tomcat.sessions.expired","tomcat.sessions.rejected"]}

If we want a specific detailed metric for example (system.cpu.usage) then 
URL:PORT/actuator/metrics/system.cpu.usage

{"name":"system.cpu.usage","description":"The \"recent cpu usage\" of the system the application is running in","measurements":[{"statistic":"VALUE","value":0.0}],"availableTags":[]}

but prometheus can understand this it expects to be in a specific format for that we need to add io.micrometer dependency

<!-- https://mvnrepository.com/artifact/io.micrometer/micrometer-registry-prometheus -->
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-registry-prometheus</artifactId>
    <version>1.13.3</version>
</dependency>

------------------------------------------
now we need to screp this metrics with prometheus to do that we need to add this application as a target in prometheus configuration to do so
go to configmas 

kubectl get cm -n prometheus
edit the prometheus-server    

 kubectl edit cm prometheus-server -n prometheus
add this in scrape_configs section 

scrape_configs:
    - job_name: petclinic-deployment-dev
      metrics_path: '/actuator/prometheus'
      scrape_interval: 3s
      static_configs:
      - targets: ['frontend-service-dev.petclinic-dev:81']
        labels:
          application: 'petclinic-deployment-dev'

- job_name: petclinic-deployment-dev  -->job name
scrape_interval: 3s  --->scarp intervel
static_configs:
- targets: ['frontend-service-dev.petclinic-dev:81']  -->endpoint url 
labels:
application: 'petclinic-deployment-dev' -->label

after editing  port-forward the prometheus server and check the status menu and click on targets check is the target is configured and up 
then go to graphs gives some prompql commands ( kubectl port-forward svc/prometheus-server 9090:80 -n prometheus)
and execute
 then try accessing your application then you will get the realtime info based on the prompt.
 
 kubectl port-forward svc/prometheus-server 9090:80 -n prometheus

Service Discovery:
we have added a target of specific application manually but as we have 100's of such(app)pods we cant add them manually so we use service Discovery
so we use service Discovery mechanism lets understand how kubernetes service Discovery works
we can also dynamically Discovery using dynamic config's 
kubernetes service Discovery scrap's targets from kubernetes API
 ref:https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config
as per the kubernetes service Discovery
annotations:
  prometheus.io/path: /actuator/prometheus
  prometheus.io/scrape: "true"

add this to to make target to prometheus dynamically in annotation 





Grafana:

Get your 'admin' user password by running:

kubectl get secret --namespace grafana grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   grafana.grafana.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:
     export POD_NAME=$(kubectl get pods --namespace grafana -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana" -o jsonpath="{.items[0].metadata.name}")
     kubectl --namespace grafana port-forward $POD_NAME 3000

3. Login with the password from step 1 and the username: admin
#################################################################################
######   WARNING: Persistence is disabled!!! You will lose your data when   #####
######            the Grafana pod is terminated.                            #####
#################################################################################    default grafana dashboadids:17375  | 6417

port-forward the grafana 

connections/dataSOURCE give the below url to integrate prometheus and grafana

http://prometheus-server.prometheus:80 
then create graph 

to send mail notification from grafana if theashold breach we need to add smtp related thing in /etc/Grafana/grafana.init
as it is a readonly file as we used helm to deploy we neeed to add the in values.yml at the time of deployment
we need to add below things at the time of deployment 

need to add but creating values.yml by using below command

 helm show values grafana/grafana > values.yml

then update or install helm chat as below 
helm upgrade grafana grafana/grafana -f values.yml --namespace grafana
## Grafana SMTP Configuration
smtp:
  # Enable SMTP
  enabled: true
  # SMTP server configuration
  host: smtp.gmail.com:465
  # SMTP user credentials
  user: your-email@example.com
  password: your-email-password
  # SMTP settings
  fromAddress: your-email@example.com
  fromName: Grafana
  # Optional: Enable TLS/SSL for SMTP
  skipVerify: false
  
  RBAC --->
  first create 2 users  named Kumar who works on DEV namespace rescources and Murthy who Works on Prod namespace rescources i want kumar to have read permissions on all the rescources on all the namespaces
  but we  also need Kumar to have delete modify permissions on Dev-namespace as he works on that namespace
  same with Murthy as he works on Prod we need to give delete create and modfiy access on Prod 

so we need to add kumar to kubernetes using iamindentitymapping
 eksctl create iamidentitymapping --cluster eks-cluster --arn arn:aws:iam::021891605639:user/Kumar --username Kumar --group Dev-admin
 eksctl create iamidentitymapping --cluster eks-cluster --arn arn:aws:iam::021891605639:user/Murthy --username Murthy --group Prod-admin
 
the IAM identity mapping integrates IAM users and roles with Kubernetes users and groups, allowing for a seamless 
combination of AWS IAM and Kubernetes RBAC (Role-Based Access Control).


 next create and add kumar in aws-auth Configmap 
 apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    []  
  mapUsers: |
    - userarn: arn:aws:iam::021891605639:user/kumar
      username: kumar
      groups:
        - Dev-admin


 The aws-auth ConfigMap is necessary to map IAM users or roles to Kubernetes users and groups. Without this step, Kubernetes won't recognize the IAM users or roles and thus won't apply the permissions defined in the ClusterRoleBinding.
C:\Users\prasa>kubectl auth can-i get pods -n petclinic-prod --as Kumar
yes

C:\Users\prasa>kubectl get nodes --as Kumar
NAME                         STATUS   ROLES    AGE    VERSION
ip-10-0-3-230.ec2.internal   Ready    <none>   4d8h   v1.30.2-eks-1552ad0
ip-10-0-3-239.ec2.internal   Ready    <none>   2d3h   v1.30.2-eks-1552ad0
ip-10-0-4-216.ec2.internal   Ready    <none>   4d8h   v1.30.2-eks-1552ad0

C:\Users\prasa>kubectl auth can-i get nodes --as Kumar

yes


PS C:\Users\prasa> kubectl auth can-i delete jobs -n petclinic-dev --as Kumar
yes
PS C:\Users\prasa> kubectl auth can-i delete jobs -n petclinic-prod --as Kumar
no

need to check the same for Murthy
PS C:\Users\prasa>

----------------------------------------------------------
prometheus alerting:
--------------------
up{ service="frontend-service-dev"}
up metrics is a special type of metrics which will define wether the metrics is up or down 
if it is 1 them our service is running 
if 0 then the service is down 

we need to create a role such that when an metrics is zero then alert will be sent 
we need to config that in a cm in promethues namespace:
kubectl edit cm prometheus-server -n prometheus 

alerting rule:alerting_rules.yml: | 

groups:
    - name: example_alerts
      rules:
        - alert: FrontendServiceDown
          expr: up{service="frontend-service-dev"} == 0
          for: 5s
          labels:
            severity: critical
          annotations:
            summary: "Frontend service is down"
            description: "The frontend service ({{ $labels.service }}) has been down for more than 5 seconds."

now we need to edit the alert-manager cm file to trigger the alert 
prometheus server only checks the rule once the rule was met it triggers the alert manager  so we need to again set the notification configuration
 kubectl edit cm prometheus-alertmanager -n prometheus

 need to get the slack webhook url from 
 api.slack.com/apps 
 for the configration that have be done refer the file cmdemoprometheus 

Health probes:
--------------
Liveness Probe: Deals with long-term issues by restarting the container.
Readiness Probe: Deals with short-term issues by temporarily removing the Pod from service without restarting it.




Dependency Management: If your main application container depends on another service being available (e.g., a database or API), you can use this approach to ensure the dependent service is up and running before your application starts.

Avoiding Failures: Starting the main application container before its dependencies are ready could lead to connection errors, 
which might require complex retry logic in the application. This initContainer simplifies the process by ensuring dependencies are available before the main container starts.

Example Use Case:
Imagine you have a microservice A that depends on microservice B (myservice in this case). Service A should only start after B is available.
 This initContainer will block the startup of service A until it can confirm that service B is resolvable via DNS, ensuring that service A has a higher chance of successfully connecting to service B on its first try.

Summary:
The initContainer waits for a dependent service (myservice) to become available by checking its DNS resolution. 
This ensures that the main application container doesn't start until its dependencies are ready, improving the reliability and stability of your application's startup process.



LimitRange
The min and max values in a Kubernetes LimitRange act as boundaries for the resource requests and limits that can be set in pod 
or container specifications within a namespace. If the resources.requests or resources.limits specified in your Deployment.yaml fall
 outside these boundaries, Kubernetes will reject the pod. Additionally, if no resource requests or limits are defined in the pod spec, 
 Kubernetes will automatically apply the defaultRequest and default values from the LimitRange, as long as they fall within the min and max
  boundaries. Thus, the min and max values ensure that all resource specifications, whether explicitly set in the pod or applied by default,
   stay within acceptable limits.

