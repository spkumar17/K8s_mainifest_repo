#Get Kube config File for your EKS cluster:
---->The kubeconfig file is essential for interacting with your Amazon EKS cluster. This file contains configuration details such as the cluster's API server endpoint, authentication details, and context information, allowing you to use kubectl to manage your EKS cluster.
command:
aws eks --region us-east-1 update-kubeconfig --name eks-cluster
----------------------------------------------------------------------------------
#install eksctl:k
curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
----------------------------------------------------------------------------------------
#install helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

---->how to install in windows https://github.com/helm/helm/releases
--------------------------------------
#install CSI driver
#CSI drivers allow Kubernetes to interface with various storage backends through a standard API. 

helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver

helm repo update
helm upgrade --install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver --namespace kube-system --version v1.33.0-eksbuild.1

-----------------------------
to decrypt base64
--------------------------
powershell -Command "[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String('S0ZyZlQxYnc2dE1sbjZZUQ=='))"

Encoding to base64 in windows

$text = "password"
$bytes = [System.Text.Encoding]::UTF8.GetBytes($text)
$base64 = [Convert]::ToBase64String($bytes)
$base64

--------------------------------------
ingress nginix install

Webpage ref:https://kubernetes.github.io/ingress-nginx/deploy/


-------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-petclinic-dev
  namespace: petclinic-dev
spec:
  rules:
    - host: "a31ecb4d8de0a4e8c805c6a706e12f50-860764022.us-east-1.elb.amazonaws.com" # lb dns name as i don't have dns name
      http:
        paths:
        - pathType: Prefix
          path: "/dev"
          backend:         
            service:
              name: frontend-service-dev
              port:
                number: 80
              #This configuration ensures that requests to the path /dev on the external IP address of the Ingress Controller will be routed to the frontend-service-dev service, even though it is a ClusterIP service.

              =------------------------------------------------------

installing nginx ingress controller using helm 

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx/
helm repo update
kubectl create namespace ingress-nginx
helm install nginx-ingress ingress-nginx/ingress-nginx -n ingress-nginx

make sure controller watches all ingress rescources in all the namespaces 
modify this deployment.yml of controller:

kubectl edit deployment nginx-ingress-ingress-nginx-controller -n ingress-nginx  

spec:
  containers:
    - name: nginx-ingress-controller
      args:
        - /nginx-ingress-controller
        - --watch-namespace=

        add  - --watch-namespace=
-----
if you find any issue  make sure to check the logs of the deployment
kubectl logs nginx-ingress-ingress-nginx-controller-7855544b44-n6bcf -n ingress-nginx


--------------------------------------------------------
IRSA: ---> to make eks to access ebs 

install eks then

associate oidc provider with iam:
eksctl utils associate-iam-oidc-provider --cluster eks-cluster --approve

then create an iam role for service account 

single line:
eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster eks-cluster --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve --role-only --role-name AmazonEKS_EBS_CSI_DriverRole


eksctl create addon --name aws-ebs-csi-driver --cluster eks-cluster --service-account-role-arn arn:aws:iam::021891605639:role/AmazonEKS_EBS_CSI_DriverRole --force


as the IRSA GOT depriciated for ebs_csi_aws we need to use pod identily associate
 (IRSA has been deprecated; the recommended way to provide IAM permissions for "aws-ebs-csi-driver" addon is via pod identity associations; after addon creation is completed, run `eksctl utils migrate-to-pod-identity`)



 eksctl utils migrate-to-pod-identity --cluster eks-cluster --region us-east-1   --approve




if we have this below error follow the below commands

PS C:\Users\prasa> eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster eks-cluster --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --approve --role-only --role-name AmazonEKS_EBS_CSI_DriverRole
2024-08-15 09:48:58 [ℹ]  1 existing iamserviceaccount(s) (kube-system/ebs-csi-controller-sa) will be excluded
2024-08-15 09:48:58 [ℹ]  1 iamserviceaccount (kube-system/ebs-csi-controller-sa) was excluded (based on the include/exclude rules)
2024-08-15 09:48:58 [!]  serviceaccounts in Kubernetes will not be created or modified, since the option --role-only is used
2024-08-15 09:48:58 [ℹ]  no tasks

eksctl delete iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster eks-cluster 
 eksctl delete addon --name aws-ebs-csi-driver --cluster eks-cluster
aws eks describe-addon-versions --addon-name aws-ebs-csi-driver --kubernetes-version 1.30










----------------------------
kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm

nslookup mysql-headless-backend.petclinic-dev.svc.cluster.local
nslookup frontend-service-dev.petclinic-dev.svc.cluster.local

-----------------------
kubectl run -i --tty --image nginx:alpine test-pod --restart=Never --rm -- sh

curl http://frontend-service-dev.petclinic-dev.svc.cluster.local:81

we cannot use curl to connect to mysql
MySQL is a database server and does not communicate over HTTP by default; it uses its own protocol over TCP.
or use netcat:
nc -zv mysql-headless-backend.petclinic-dev.svc.cluster.local 3306

-----------------------------------------
instead use 
kubectl run -i --tty --image=mysql:5.0 --restart=Never --rm mysql-client -- sh

mysql -h mysql-headless-backend.petclinic-dev.svc.cluster.local -P 3306 -u your-username -p

--------------------------------------------
copy data from local machine to pods pv mount path 

kubectl cp ./mysql mysql-0:/var/lib/mysql

kubectl cp . mysql-0:/var/lib/mysql -c mysql -n petclinic-prod

enter into mysql container 
kubectl exec -it mysql-0 -n petclinic-prod -- sh
kubectl exec -it mysql-0 -n petclinic-dev -- sh

then connect to mysql client 
bash-4.2# mysql -u root -p
Enter password:

RUN THIS COMMANDS AND CREATE TABELES AND DATA IN IT 
SOURCE user.sql
USE petclinic;
SOURCE data.sql
SOURCE schema.sql
verify using below:
SHOW DATABASES;
USE petclinic;
show TABELES;
SELECT * FROM owners;
SELECT * FROM vets;

to check data is stored in pv restart the pod and rerun the following commands using mysql client
---------------------------------------------------
use this if we get an issue stating that unable to create the pod and connect to pv as there are already files in the mentioned path 
inorder to clear the files create and init container which runs before the pod and clear the files in that specific path
initContainers:
        - name: init-mysql-cleanup
          image: busybox
          command: ['sh', '-c', 'rm -rf /var/lib/mysql/*']
          volumeMounts:
            - name: mysql-storage-prod
              mountPath: /var/lib/mysql

this helps in clearing the pv dir files  if the issue happens 
---------------------------------------------------

got an issue like when ever i restart my frontend pod all the data i entered is deleting it is not becouse the backend is not correctly configered 
but application.properties Database is configered as H2 which is in-memory database which is not persistent whenever pod restarted data will be lost
so i changed it to MYSQL and so data is now getting stored in pv even if any frentend or backend pod get restarted data will never get lost until we delete the pv manully 
# Activate the MySQL profile this will change the default H2 to MYSQL same need to change to any other databases 
spring.profiles.active=mysql

and missconfigured the svc in deployoment manifest resulting in prod rescources saving data in dev mysql to over come this we neede to use RBAC
Even if we missconfigured we need to male sure that dev rescources should not communicate with prod rescources.

make sure to  config configmaps and servers in order to get error 

this we faces:
ImagePullBackOff
CrashLoopBackOff
ConfigMapError
Createcontainerconfigerror

Prometeus ans grafana:
----------------------
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.prometheus.svc.cluster.local
Prometheus server URL
kubectl --namespace prometheus port-forward $POD_NAME 9090
alert manager
kubectl --namespace prometheus port-forward $POD_NAME 9093
The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.prometheus.svc.cluster.local
 PushGateway URL 
 kubectl --namespace prometheus port-forward $POD_NAME 9091

 ------------------------
 this command checks the cpu and memory ultilization of nodes 
C:\Users\prasa>kubectl top nodes
NAME                         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-0-3-230.ec2.internal   34m          1%     1318Mi          39%
ip-10-0-4-216.ec2.internal   28m          1%     1124Mi          33%


Grafana:

Get your 'admin' user password by running:

kubectl get secret --namespace grafana grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   grafana.grafana.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:
     export POD_NAME=$(kubectl get pods --namespace grafana -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana" -o jsonpath="{.items[0].metadata.name}")
     kubectl --namespace grafana port-forward $POD_NAME 3000

3. Login with the password from step 1 and the username: admin
#################################################################################
######   WARNING: Persistence is disabled!!! You will lose your data when   #####
######            the Grafana pod is terminated.                            #####
#################################################################################

   